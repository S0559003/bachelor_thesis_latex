@book{DieterSchmalstieg2016,
author = {Schmalstieg, Dieter and H{\"{o}}llerer, Tobias},
isbn = {978-0-321-88357-5},
pages = {496},
title = {{Augmented Reality: Principles and practice}},
year = {2016}
}
@misc{Schanze2018,
author = {Schanze, Robert},
title = {{Was ist Mixed Reality? â€“ Unterschied zu Virtual & Augmented Reality erkl{\"{a}}rt}},
url = {https://www.giga.de/extra/mixed-reality/},
urldate = {2019-05-17},
year = {2018}
}
@book{MichaelRichter2016,
author = {Richter, Michael and Fl{\"{u}}cker, Markus},
isbn = {978-3-662-49827-9},
pages = {219},
title = {{Usability und UX kompakt Produkte f{\"{u}}r Menschen}},
year = {2016}
}
@unpublished{PaulMilgram1994,
author = {Milgram, Paul and KISHINO, Fumio},
title = {{A Taxonomy of Mixed Reality Visual Displays}},
url = {https://search.ieice.org/bin/summary.php?id=e77-d_12_1321},
year = {1994}
}
@book{Nielsen1994,
author = {Nielsen, Jakob},
isbn = {0125184069},
pages = {362},
title = {{Usability Engineering}},
url = {https://books.google.de/books?hl=de&lr=&id=DBOowF7LqIQC&oi=fnd&pg=PP1&dq=usability&ots=Bl5aUOFUCN&sig=8apn_0ZCNfu_NhbGpjnUGKTgyjY#v=onepage&q=usability&f=false},
year = {1993}
}
@book{Spiekermann2016,
author = {Spiekermann, Sarah},
isbn = {978-1-4822-2635-5},
pages = {257},
title = {{Ethical IT Innovation}},
year = {2016}
}
@book{RexHartson2012,
author = {Hartson, Rex and {S. Payla}, Pardha},
isbn = {978-0-12-385241-0},
pages = {937},
title = {{The UX Book}},
year = {2012}
}
@book{MaryBethRossonJohnM.CarrollDianeD.Cerra2002,
author = {Rosson, Mary Beth and Carroll, John M. and Cerra, John M. and Hill, Natalie},
isbn = {1-55860-712.9},
title = {{Usability Engineering: Scenario-Based Development of Human-Computer}},
year = {2001}
}
@unpublished{XiangyuWangNingGu2008,
author = {Wang, Xiangyu and Gu, Ning and Marchant, David},
pages = {17},
title = {{AN EMPIRICAL STUDY ON DESIGNERS' PERCEPTIONS OF AUGMENTED REALITY WITHIN AN ARCHITECTURAL FIRM}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.212.1910&rep=rep1&type=pdf},
year = {2008}
}
@book{Tonnis2010,
author = {T{\"{o}}nnis, Marcus},
file = {:C\:/Users/alioi/OneDrive/Desktop/T{\"{o}}nnis_2010_Book_Einbilicke_in_Augmented_Reality.pdf:pdf},
isbn = {978-3-642-14178-2},
pages = {199},
title = {{Augmented Reality Einblicke in die Erweiterte Realit{\"{a}}t}},
year = {2010}
}
@unpublished{HolgerRegenbrechtGregoryBaratoff2005,
author = {Regenbrecht, Holger and Baratoff, Gregory and Wilke, Wilhelm},
pages = {56},
title = {{Augmented Reality Projects in the Automotive and Aerospace Industries}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1528433},
year = {2005}
}
@misc{PTC,
author = {PTC},
title = {{Model Targets Supported Objects & CAD Model Best Practices}},
url = {https://library.vuforia.com/content/vuforia-library/en/articles/Solution/model-targets-supported-objects.html},
urldate = {2019-06-05}
}
@unpublished{Rodden1992,
author = {Rodden, Tom},
pages = {33},
title = {{A Survey of CSCW Systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.8704&rep=rep1&type=pdf},
year = {1992}
}
@unpublished{Shneiderman1996,
author = {Shneiderman, Ben},
pages = {8},
title = {{The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations}},
url = {https://www.cs.umd.edu/$\sim$ben/papers/Shneiderman1996eyes.pdf},
year = {1996}
}
@inproceedings{ElSayedNevenA.M.BruceH.ThomasRossT.Smith2015,
author = {{Neven A. M.}, ElSayed and Thomas, Bruce H. and Smith, Ross T. and Marriott, Kim},
booktitle = {IEEE Virtual Reality Conference 2015},
pages = {2},
title = {{Using Augmented Reality to Support Situated Analytics}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7223352&tag=1},
year = {2015}
}
@unpublished{DanielA.KeimFlorianMansmannJornSchneidewindJimThomas2008,
author = {Keim, Daniel A. and Mansmann, Florian and Schneidewind, Joern and Thomas, Jim and Ziegler, Hartmut},
pages = {15},
title = {{Visual Analytics: Scope and Challenges}},
url = {http://kops.uni-konstanz.de/bitstream/handle/123456789/5631/Visual_Analytics_Scope_and_Challenges.pdf?sequence=1&isAllowed=y},
year = {2008}
}
@unpublished{Azuma2001,
author = {Azuma, Ronald and Baillot, Yohan and Behringer, Reinhold and Feiner, Steven and Julier, Simon and MacIntyre, Blair},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2001 - (No Title).pdf:pdf},
title = {{Recent Advances in Augmented Reality}},
url = {www.hitl.washington.edu/research/shared_space/.},
year = {2001}
}
@book{RexHartson2012,
author = {Hartson, Rex and {S. Payla}, Pardha},
isbn = {978-0-12-385241-0},
pages = {937},
title = {{The UX Book}},
year = {2012}
}
@unpublished{Gabbard,
abstract = {A major challenge, and thus opportunity, in the field of human-computer interaction and specifically usability engineering is designing effective user interfaces for emerging technologies that have no established design guidelines or interaction metaphors or introduce completely new ways for users to perceive and interact with technology and the world around them. Clearly, augmented reality is one such emerging technology. We propose a usability engineering approach that employs user-based studies to inform design, by iteratively inserting a series of user-based studies into a traditional usability engineering lifecycle to better inform initial user interface designs. We present an exemplar user-based study conducted to gain insight into how users perceive text in outdoor augmented reality settings and to derive implications for design in outdoor augmented reality. We also describe "lessons learned" from our experiences conducting user-based studies as part of the design process.},
author = {Gabbard, Joseph L and {Edward Swan}, J II},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gabbard, Edward Swan - Unknown - Usability Engineering for Augmented Reality Employing User-based Studies to Inform Design.pdf:pdf},
keywords = {Augmented,Evaluation / Methodology,H51: Multimedia Information Systems-Artificial,Index Terms-H52: User Interfaces-Ergonomics,Screen Design,Style Guides,and Virtual Realities},
title = {{Usability Engineering for Augmented Reality: Employing User-based Studies to Inform Design}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.424.308&rep=rep1&type=pdf},
year = {2008}
}
@unpublished{Shneiderman1996a,
abstract = {A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). Everything points to the conclusion that the phrase 'the language of art' is more than a loose metaphor, that even to describe the visible world in images we need a developed system of schemata. E. H. Gombrich Art and Illusion, 1959 (p. 7 6) keys), are being pushed aside by newer notions of information gathering, seeking, or visualization and data mining, warehousing, or filtering. While distinctions are subtle, the common goals reach from finding a narrow set of items in a large collection that satisfy a well-understood information need (known-item search) to developing an understanding of unexpected patterns within the collection (browse) (Marchionini, 1995). Exploring information collections becomes increasingly difficult as the volume grows. A page of information is easy to explore, but when the information becomes the size of a book, or library, or even larger, it may be difficult to locate known items or to browse to gain an overview, Designers are just discovering how to use the rapid and high resolution color displays to present large amounts of information in orderly and user-controlled ways. Perceptual psychologists, statisticians, and graphic designers (Berlin, 1983; Cleveland, 1993; Tufte, 1983, 1990) offer valuable guidance about presenting static information, but the opportunity for dynamic displays takes user interface designers well beyond current wisdom.},
author = {Shneiderman, Ben},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shneiderman - Unknown - The Eyes Have It A Task by Data Type Taxonomy for Information Visualizations.pdf:pdf},
title = {{The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations}},
url = {https://www.cs.umd.edu/$\sim$ben/papers/Shneiderman1996eyes.pdf},
year = {1996}
}
@unpublished{Tatzgern2016,
abstract = {Figure 1: (a) A user looking for a book to read can use the glyph visualization to compare items and identify interesting books. However, finding the real world location of books is difficult because of the clutter. (b) With an adaptive information density display, the user has a better overview of relevant books. ABSTRACT Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. Clustering items by spatial and semantic attributes can temporarily alleviate the issue, but is not effective against an increasing amount of data. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Our method selects items and groups from different levels of this hierarchy based on user-defined preferences and on the amount of visual clutter caused by placing these items. The number of presented items is adapted during user interaction to avoid clutter. We compare our interface to a conventional AR browser interface in a qualitative user study. Users clearly preferred our interface, because it provided a better overview of the data and allowed for easier comparison. In a second study, we evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.},
author = {Tatzgern, Markus and Orso, Valeria and Kalkofen, Denis and Jacucci, Giulio and Gamberini, Luciano and Schmalstieg, Dieter},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tatzgern et al. - Unknown - Adaptive Information Density for Augmented Reality Displays.pdf:pdf},
title = {{Adaptive Information Density for Augmented Reality Displays}},
url = {https://help.here.com/de/wp8/citylens},
year = {2016}
}
@unpublished{Bell2001,
abstract = {We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible. We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the unoccupied space in which objects can be placed to Pen$\sim$ission to make digital or hard copies of all o1" part of this work for personal o1" classroom use is granted without tee provided that copies arc not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific pemaission and/or a fee. UISI' 01 Orlando FLA Copyright ACM 2001 1-58113-438-x/01/l 1...$5.00 avoid occlusion. Layout decisions from previous frames are taken into account to reduce visual discontinuities. We present augmented reality and virtual reality examples to which we have applied our approach, including a dynamically labeled and annotated environment.},
author = {Bell, Blaine and Feiner, Steven and Hsllerer, Tobias},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bell, Feiner, Hsllerer - 2001 - View Management for Virtual and Augmented Reality.pdf:pdf},
keywords = {136 [Computer Graphics] Methodology and Techniques,137 [Computer Graphics] Three-Dimensional Graphics,CR Categories and Subject Descriptors: H51 [Inform,H52 [Information Interfaces and Presentation] User,Screen design,and virtual realities,annotation,augmented,augmented reality,environment management,labeling,virtual environments,wearable computing},
title = {{View Management for Virtual and Augmented Reality}},
url = {https://dl.acm.org/citation.cfm?id=502363},
year = {2001}
}
@misc{Grasset2012,
author = {Grasset, Raphael and Langlotz, Tobias and Kalkofen, Denis and Tatzgern, Markus and Schmalstieg{\P}, Dieter},
title = {{IEEE Xplore Full-Text PDF:}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6402555},
urldate = {2019-08-26},
year = {2012}
}
@unpublished{Polys,
abstract = {Increasingly designers, engineers, and scientists require 'Integrated Information Spaces' where spatial, abstract, and temporal data are simultaneously available and linked. To address this problem in our work we are developing Information-Rich Virtual Environments (IRVEs). An IRVE combines the capabilities of virtual environments and information visualization to support the integrated exploration of spatial, abstract, and temporal data This paper enumerates the display and interaction requirements for Integrated Information Spaces and describes our first attempts to meet them in the context of international standards such as XML, VRML, and X3D. We will describe our IRVE designs and implementations from a user-centered, usability engineering perspective and detail their strengths and weaknesses for publishing and service architectures. We have prototyped systems to unify heterogeneous information types for applications in cheminformatics and biomedical simulation. Current standards promise to enable better IRVEs including X3D's support for Metadata and the proposed components for Annotation and Compositing. Finally, we examine how our work can inform the standards design process to insure that crucial IRVE functionality can be supported. 1.0 Introduction Across a wide variety of domains, analysts and designers are faced with complex systems that include spatial objects, their attributes and their dynamic behaviors. In order to study and understand these systems, users need a unified platform to explore the complex relationships between their heterogeneous data types. Next generation digital tools must address this need for integration of spatial, abstract, and temporal information; we have described this as the goal of 'Information-Rich Virtual Environments' [Bowman et al, 2003]. Consider an engineer, 'E', working on a complex aerospace craft such as the Space Shuttle. The craft is aging and the tolerances of the original parts are suspect. The engineer is tasked with designing a new gear assembly for the tailfin brake. E builds a 3D geometric model of the assembly in a CAD program, specifying its dimensions and the materials for its construction. E must then test the assembly design and its parts for physical tolerances using a meshing program and finite-element simulator. E must specify the kinetic forces of the assembly such as gears, locks, fulcri etc. After the simulator is run, E analyzes the results looking for weak points and insuring that all parts are within physical requirements. E repeats this process a number of times to satisfy the specified design constraints of material's weights and stress limits. When he is satisfied, he saves the candidate model and simulation results into a database that represents the craft in its entirety. But E is not done; he must also confirm how his design affects the whole craft in flight and damage scenarios. E's new part is then evaluated in the context of the other shuttle systems. Each scenario is linked to prioritized causal chains in a knowledgebase that infers mission consequences due to the particular flight/damage scenario. How many applications did E use? How many times was he required to switch between a spatial view (e.g. a design application), a temporal view (e.g. a simulator application), and an abstract view (e.g. a information visualization application)? Was any model data lost or added between the applications? Was he able to view the impacts of his design changes simultaneously within one environment? On the same machine? This scenario illustrates the problem of integrated information spaces-current data models, applications, and presentations are fragmented and inefficient for users. Virtual environments (VEs) excel at providing users a greater comprehension of spatial objects, their perceptual properties and their relations. Perceptual information includes 3D spaces that represent physical or virtual objects and phenomena including geometry, lighting, colors, and textures. As users navigate within a complex VE model, they may need access to information related to the world and objects in the space (such as name, function, attributes, etc.). This is the domain of Information Visualization, which is concerned with improving how users perceive, understand, and interact with visual representations of abstract information [Card et al, 1999]. This abstract (or symbolic) information could include text, links, numbers, graphical plots, and audio/video annotations. Both perceptual and abstract information may change over time reflecting its temporal aspects. Unfortunately, most systems do not allow users flexible exploration and examination of dynamic abstract information in conjunction with a dynamic VE.},
author = {Polys, Nicholas F and Bowman, Doug A and North, Chris},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Polys, Bowman, North - Unknown - Information-Rich Virtual Environments Challenges and Outlook NASA Virtual Iron Bird Workshop, 2004.pdf:pdf},
title = {{Information-Rich Virtual Environments: Challenges and Outlook NASA Virtual Iron Bird Workshop, 2004}}
}
@unpublished{Polys2004,
abstract = {Information-Rich Virtual Environments (IRVEs) have been described as environments in which perceptual information is enhanced with abstract (or symbolic) information such as text, numbers, images, audio, video, or hyperlinked resources. Desktop VE applications present the same information design and layout challenges as immersive VEs, but in addition, they may also be integrated with external windows or frames commonly used in desktop interfaces. This paper enumerates design approaches for the display of enhancing information both internal and external to the virtual world's render volume. Using standard web-based software frameworks, we explore a number of implicit and explicit spatial layout methods for the display and linking of abstract information, especially text. Within the virtual environment view, we demonstrate both Heads-Up-Displays and encapsulated scenegraph behaviors we call Semantic Objects. For desktop displays, which support information display venues external to the scene, we demonstrate the linking and integration of the scene with web browsers and the Snap-Together visualization a system. Finally, we describe the application of these techniques in the PathSim Visualizer, an IRVE interface for the biomedical domain. These design techniques are relevant for instructional and informative interfaces for a wide variety of desktop VE applications.},
author = {Polys, Nicholas F and Bowman, Doug A},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Polys, Bowman - Unknown - Design and Display of Enhancing Information in Desktop Information-Rich Virtual Environments Challenges and Te.pdf:pdf},
keywords = {desktop virtual environments,information psychophysics,information-rich virtual environments,multiple view architectures,visualization design},
title = {{Design and Display of Enhancing Information in Desktop Information-Rich Virtual Environments: Challenges and Techniques}},
year = {2004}
}
@unpublished{Lebeck2018,
abstract = {Immersive augmented reality (AR) technologies are becoming a reality. Prior works have identified security and privacy risks raised by these technologies, primarily considering individual users or AR devices. However, we make two key observations: (1) users will not always use AR in isolation, but also in ecosystems of other users, and (2) since immersive AR devices have only recently become available, the risks of AR have been largely hypothetical to date. To provide a foundation for understanding and addressing the security and privacy challenges of emerging AR technologies, grounded in the experiences of real users, we conduct a qualitative lab study with an immersive AR headset, the Microsoft HoloLens. We conduct our study in pairs-22 participants across 11 pairs-wherein participants engage in paired and individual (but physically co-located) HoloLens activities. Through semi-structured interviews, we explore participants' security, privacy, and other concerns, raising key findings. For example, we find that despite the HoloLens's limitations, participants were easily immersed, treating virtual objects as real (e.g., stepping around them for fear of tripping). We also uncover numerous security, privacy, and safety concerns unique to AR (e.g., deceptive virtual objects misleading users about the real world), and a need for access control among users to manage shared physical spaces and virtual content embedded in those spaces. Our findings give us the opportunity to identify broader lessons and key challenges to inform the design of emerging single-and multiuser AR technologies.},
author = {Lebeck, Kiron and Ruth, Kimberly and Kohno, Tadayoshi and Roesner, Franziska and Allen, Paul G},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lebeck et al. - 2018 - Towards Security and Privacy for Multi-User Augmented Reality Foundations with End Users.pdf:pdf},
title = {{Towards Security and Privacy for Multi-User Augmented Reality: Foundations with End Users}},
url = {https://ar-sec.cs.washington.edu},
year = {2018}
}
@unpublished{Bimber2005,
abstract = {This copy was downloaded from SpatialAR.com for individual use. Save 20% on the printed copy of this book when you order online at www.akpeters.com. Use discount code spatialar.},
author = {Bimber, Oliver and Raskar, Ramesh},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bimber, Raskar - 2005 - Spatial Augmented Reality Merging Real and Virtual Worlds.pdf:pdf},
title = {{Spatial Augmented Reality Merging Real and Virtual Worlds}},
url = {www.akpeters.com.},
year = {2005}
}
@unpublished{Willett2017,
abstract = {Fig. 1. Examples of embedded data representations 1 : (a) dye-based flow visualization on a 1/48 scale airplane model, (b) Yelp's Monocle application uses a mobile phone to display business ratings in front of the establishments, (c) concept image of an augmented reality visualization of urban wind flow [42], (d) a visualization of wifi signal strength made with a moving LED rod and long exposure photography, (e) MRI overlay for joint arthrography [13], and (f) a concept image of a drone swarm visualizing crop health [51]. Abstract-We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents-the real-world entities and spaces to which data corresponds-and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualiza-tion, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and phys-icalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.},
author = {Willett, Wesley and Jansen, Yvonne and Dragicevic, Pierre},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Willett, Jansen, Dragicevic - Unknown - Embedded Data Representations.pdf:pdf},
keywords = {Index Terms-Information visualization,ambient displays,augmented reality,data physicalization,ubiquitous computing},
title = {{Embedded Data Representations}},
year = {2017}
}
@unpublished{Azuma1997,
abstract = {This paper surveys the field of Augmented Reality, in which 3-D virtual objects are integrated into a 3-D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment and military applications that have been explored. This paper describes the characteristics of Augmented Reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective Augmented Reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using Augmented Reality.},
author = {Azuma, Ronald T},
booktitle = {Presence: Teleoperators and Virtual Environments},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Azuma - 1997 - A Survey of Augmented Reality.pdf:pdf},
pages = {355--385},
title = {{A Survey of Augmented Reality}},
url = {http://www.cs.unc.edu/$\sim$azumaW:},
volume = {6},
year = {1997}
}
@book{Kalkofen2007,
abstract = {Figure 1 An example of an enhanced augmentation. Focus objects (in red) are not only overlaid on top of the video image, but they are partially occluded by key features from context objects. This provides object occlusion with key features of occluding objects. A second level context (yellow seats) further helps an understanding of the scene. Edges in this image are enhanced considering occlusions with other objects. This helps us to better control the depth complexity of the scene ABSTRACT In this article we present interactive Focus and Context (F+C) visualizations for Augmented Reality (AR) applications. We demonstrate how F+C visualizations are used to affect the user's perception of hidden objects by presenting contextual information in the area of augmentation. We carefully overlay synthetic data on top of the real world imagery by taking into account the information that is about to be occluded. Furthermore, we present operations to control the amount of augmented information. Additionally, we developed an interaction tool, based on the Magic Lens technique, which allows for interactive separation of focus from context. We integrated our work into a rendering framework developed on top of the Studierstube Augmented Reality system. We finally show examples to demonstrate how our work benefits AR.},
author = {Kalkofen, Denis and Mendezt, Erick and Schmalstiegt, Dieter},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalkofen, Mendezt, Schmalstiegt - Unknown - Interactive Focus and Context Visualization for Augmented Reality.pdf:pdf},
isbn = {9781424417506},
keywords = {136 [Methodology and Techniques]: Interaction tech,Categories H5 1 [Multimedia Information Systems]:,E13 [Data Structures]: Graphs and Networks,Graphics data structures and data types Keywords O,H52 [User Interfaces]: Style guides,and virtual realities,augmented,interaction techniques for MR/AR,mediated and diminished reality,real-time rendering},
title = {{Interactive Focus and Context Visualization for Augmented Reality}},
url = {http://www.cs.tufts.edu/comp/250VIS/papers/AR-FC-VIS.pdf},
year = {2007}
}
@book{Marriott2018,
author = {Marriott, Kim and Schreiber, Falk and Klein, {\textperiodcentered} Karsten and Riche, Nathalie Henry and Itoh, Takayuki and Stuerzlinger, Wolfgang and (Eds.), Bruce H. Thomas},
file = {:C\:/Users/alioi/OneDrive/Desktop/2018_Book_ImmersiveAnalytics.pdf:pdf},
isbn = {978-3-030-01387-5},
pages = {366},
title = {{Immersive Analytics}},
year = {2018}
}
@phdthesis{Brandenburg2019,
author = {Brandenburg, Elisabeth},
file = {:C\:/Users/alioi/OneDrive/Desktop/Dissertation_Brandenburg_2019.pdf:pdf},
pages = {231},
publisher = {Fraunhofer Verlag},
school = {Technische Universit{\"{a}}t Berlin},
title = {{Gestaltungsrichtlinien f{\"{u}}r die Visualisierung von Produktinformationen in der virtuellen Umgebung CAVE zur Unterst{\"{u}}tzung von Design Review Teams}},
type = {Dissertation},
year = {2019}
}
@unpublished{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
author = {Klein, Georg and Murray, David},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klein, Murray - Unknown - Parallel Tracking and Mapping for Small AR Workspaces.pdf:pdf},
title = {{Parallel Tracking and Mapping for Small AR Workspaces}},
year = {2007}
}
@techreport{Templeman2012,
abstract = {As smartphones become more pervasive, they are increasingly targeted by malware. At the same time, each new generation of smartphone features increasingly powerful onboard sensor suites. A new strain of 'sensor malware' has been developing that leverages these sensors to steal information from the physical environment-e.g., researchers have recently demonstrated how malware can 'listen' for spoken credit card numbers through the microphone, or 'feel' keystroke vibrations using the accelerometer. Yet the possibilities of what malware can 'see' through a camera have been understudied. This paper introduces a novel 'visual malware' called PlaceRaider, which allows remote attackers to engage in remote reconnaissance and what we call "virtual theft." Through completely opportunistic use of the phone's camera and other sensors, PlaceRaider constructs rich, three dimensional models of indoor environments. Remote burglars can thus 'download' the physical space, study the environment carefully, and steal virtual objects from the environment (such as financial documents, information on computer monitors, and personally identifiable information). Through two human subject studies we demonstrate the effectiveness of using mobile devices as powerful surveillance and virtual theft platforms, and we suggest several possible defenses against visual malware.},
archivePrefix = {arXiv},
arxivId = {1209.5982v1},
author = {Templeman, Robert and Rahman, Zahid and Crandall, David and Kapadia, Apu},
eprint = {1209.5982v1},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Templeman et al. - 2012 - PlaceRaider Virtual Theft in Physical Spaces with Smartphones.pdf:pdf},
title = {{PlaceRaider: Virtual Theft in Physical Spaces with Smartphones}},
year = {2012}
}
@techreport{Roesner2013,
author = {Roesner, Franziska and Kohno, Tadayoshi and Molnar, David},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roesner, Kohno, Molnar - Unknown - Security and Privacy for Augmented Reality Systems.pdf:pdf},
title = {{Security and Privacy for Augmented Reality Systems}},
url = {http://www.flickr.com/photos/neven/5269418871/},
year = {2013}
}
@phdthesis{Kirschner2012,
author = {Kirschner, Rafael Johannes},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirschner - Unknown - Methodische Offene Produktentwicklung.pdf:pdf},
title = {{Methodische Offene Produktentwicklung}},
url = {https://mediatum.ub.tum.de/doc/1094898/1094898.pdf},
year = {2012}
}
@phdthesis{Kirschner2011,
abstract = {iman solutions GmbH, Munich Despite the widely accepted and proven benefits of open innovation methods, companies apply them infrequently in their own product development processes. In order to find reasons for this apparently contradiction, we conducted 34 interviews with project directors and a literature study. Thus, we identified ten common obstacles for customer integration. On this basis, we derived a new open innovation method to get over these hurdles. The core idea of the presented crowd sourcing method is a product picture based access to a comment management database by the product user. To illuminate the method, we present an implementation together with its internal data structure for a better understanding. This tool is then validated in an experiment with n=48 users. The results indicate that the picture based approach of the method generates valuable results that can contribute to product development.},
author = {Kirschner, Rafael and Kain, Andreas and Lang, Alexander and Lindemann, Udo},
booktitle = {INTERNATIONAL CONFERENCE ON ENGINEERING DESIGN},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirschner et al. - 2011 - IMMERSIVE PRODUCT IMPROVEMENT IPI _ FIRST EMPIRICAL RESULTS OF A NEW METHOD.pdf:pdf},
keywords = {customer integration,design method,open innovation,product improvement},
pages = {11--15},
title = {{IMMERSIVE PRODUCT IMPROVEMENT IPI _ FIRST EMPIRICAL RESULTS OF A NEW METHOD}},
year = {2011}
}
@unpublished{Poth2017,
author = {Poth, Anna},
doi = {10.13140/RG.2.2.34331.16168},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poth - Unknown - Evaluierung von Augmented Reality Frameworks anhand einer AR-Anwendung im IoT-Bereich.pdf:pdf},
title = {{Evaluierung von Augmented Reality Frameworks anhand einer AR-Anwendung im IoT-Bereich}},
url = {https://www.researchgate.net/publication/319406434},
year = {2017}
}
@book{Mayhew1999,
author = {Mayhew, Deborah J.},
isbn = {1-55860-561-4},
title = {{The usability engineering lifecycle}},
year = {1999}
}
@article{Herpich2017,
abstract = {This paper aims to analyze the existing frameworks that may allow the development of educational solutions using augmented reality resources, focusing on tools that enable the conception, design and implementation of mobile applications. As a result of this research, several development environments available on the market that facilitate working with augmented reality elements in mobile devices were discovered and examined. Among these platforms , eleven were selected to be analysed and presented in this paper. These were tested and compared to one another, and their main characteristics were indicated in a comparative table, as well as their resources that hold the potential to contribute with the construction of educational applications. All of this in favor of easing the development of applications that may assist educators in introducing augmented reality technologies in their classrooms.},
author = {Herpich, Fabr{\'{i}}cio and {Martins Guarese}, Renan Luigi and Margarida, Liane and Tarouco, Rockenbach},
doi = {10.4236/ce.2017.89101},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herpich et al. - 2017 - A Comparative Analysis of Augmented Reality Frameworks Aimed at the Development of Educational Applications.pdf:pdf},
keywords = {Augmented Reality (AR),Educational Applications,Edutainment,Mobile Devices,Software Development Kit (SDK),Virtual Reality (VR)},
pages = {1433--1451},
title = {{A Comparative Analysis of Augmented Reality Frameworks Aimed at the Development of Educational Applications}},
url = {http://www.scirp.org/journal/ce},
volume = {8},
year = {2017}
}
@unpublished{Arora2017,
abstract = {Augmented reality has been an interesting area of research for the last two decades or so. This paper presents a brief overview of the recent literature on tracking methods used in Augmented Reality applications, both for indoor and outdoor environments.},
author = {Arora, Bhavna and Parkar, Nida},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Parkar - Unknown - Augmented Reality Tracking Methods Archita Dad.pdf:pdf},
keywords = {Augmented reality,Indoor,Outdoor,tracking},
title = {{Augmented Reality: Tracking Methods Archita Dad}},
url = {www.ijert.org},
year = {2017}
}
@misc{Luber,
author = {Luber, Stefan},
title = {{Was ist ein Beacon?}},
url = {https://www.ip-insider.de/was-ist-ein-beacon-a-665274/},
urldate = {2019-08-31}
}
@unpublished{Cukovic2015,
abstract = {In this paper we present novel methods and algorithms for augmenting a foot 3D model in real-time. By overlaying reconstructed anatomical contents onto user's field of view via an augmented reality visualization platform, the virtual Biomedical Computer Aid Design (BioCAD) objects, appear geo-referenced to the real environment, help physicians and/or students to improve the overall image of the bone condition and anatomical structures of patients. Moreover, we explain the principle of marker and markerless Augmented Reality (AR) and interactive integration of virtual objects in the actual scene by development of a PC and mobile AR applications. In particular, this should be a powerful tool to understand the complex 3D nature and structure of 3D foot bones, joints and assemblies.},
author = {{\'{C}}ukovi{\'{c}}, Sa{\v{s}}a and Gattullo, Michele and Pankratz, Frieder and Deved{\v{z}}i{\'{c}}, Goran and Carrabba, Ernesto and Baizid, Khelifa},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/{\'{C}}ukovi{\'{c}} et al. - Unknown - Marker Based vs. Natural Feature Tracking Augmented Reality Visualization of the 3D Foot Phantom.pdf:pdf},
isbn = {9781941968062},
keywords = {Android,Augmented Reality,BioCAD,CT Reconstruction,DICOM,Foot 3D Model},
title = {{Marker Based vs. Natural Feature Tracking Augmented Reality Visualization of the 3D Foot Phantom}},
year = {2015}
}
@unpublished{Lowney2016,
abstract = {In this paper we describe our methods for implementing real-time edge based tracking on an Android tablet. We implement a basic tracking algorithm, chosen for both its simplicity and speed.. The performance of our system was evaluated and its advantages and flaws have been pointed out. Suggestions for further improvement are made.},
author = {Lowney, Michael and Raj, Abhilash Sunder},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowney, Raj - 2016 - Model Based Tracking for Augmented Reality on Mobile Devices.pdf:pdf},
title = {{Model Based Tracking for Augmented Reality on Mobile Devices}},
year = {2016}
}
@unpublished{Vincent2013,
abstract = {Figure 1. Handheld AR cursor-based pointing: (A) Pointing at digital marks on a physical wall map; (B) Screen-centered crosshair pointing; (C) Relative pointing with cursor stabilized in the physical object's (image) frame. (D) Spatial relations of the on-screen content in handheld AR. ABSTRACT Handheld Augmented Reality relies on the registration of digital content on physical objects. Yet, the accuracy of this registration depends on environmental conditions. It is therefore important to study the impact of registration jitter on interaction and in particular on pointing at augmented objects where precision may be required. We present an experiment that compares the effect of registration jitter on the following two pointing techniques: (1) screen-centered crosshair pointing; and (2) relative pointing with a cursor bound to the physical object's frame of reference and controlled by indirect relative touch strokes on the screen. The experiment considered both tablet and smartphone form factors. Results indicate that relative pointing in the frame of the physical object is less error prone and is less subject to registration jitter than screen-centered crosshair pointing.},
author = {Vincent, Thomas and Nigay, Laurence and Kurata, Takeshi},
doi = {10.1145/2534903.2534905},
file = {:C\:/Users/alioi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent, Nigay, Kurata - Unknown - Handheld Augmented Reality Effect of registration jitter on cursor-based pointing techniques.pdf:pdf},
keywords = {Handheld Augmented Reality,Jitter ACM Classification Keywords H52 Information,Pointing Technique,input devices and strategies},
title = {{Handheld Augmented Reality: Effect of registration jitter on cursor-based pointing techniques}},
url = {http://dx.doi.org/10.1145/2534903.2534905},
year = {2013}
}
@book{Ortega2016,
author = {Ortega, Francisco R. and Rishe, Napthali and Abyarjoo, Fatemeh and Adjouadi, Malek and Barreto, Armando},
isbn = {978-1-4822-1694-3},
pages = {763},
publisher = {CRC Press},
title = {{Interaction Design for 3D User Interfaces}},
year = {2016}
}
@unpublished{Vincent2014,
author = {Vincent, Thomas},
file = {:C\:/Users/alioi/OneDrive/Desktop/Handheld Augmented Reality_Doctor_Thesis_Vincent_Thomas_2014.pdf:pdf},
title = {{Handheld Augmented Reality Interaction: Spatial Relations}},
year = {2014}
}
@book{Bowman2011,
author = {Bowman, Doug A. and Kruijff, Ernst and {LaViola JR.}, Joseph and Poupyrev, Ivan},
isbn = {0-201-75867-9},
title = {{3D User Interfaces Theory and Practice}},
year = {2011}
}
